{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Galaxy and Non-Galaxy Classifiers:\n",
        "\n",
        "Here, we developed five machine learning classifiers for [Galaxy Zoo 2](https://data.galaxyzoo.org/#section-7) (GZ2) cataloge images to recognize the galaxy images from non-galaxy images. These five classification models use the morphological and shape information. We applied the threshold based tasks (features or disk fraction, edge-on no fraction, spiral fraction, smooth fraction, completely round fraction, odd no fraction, and odd yes fraction) to collect 780 galaxy sample images. Also, we used threshold tasks (star or artifact fraction) to collect 545 non-galaxy images. \n",
        "\n",
        "- Two classifier models, including support vector machine (SVM) and classic 1D-convolutional neural network (1D-CNN), have been designed to use the Zernike moments (ZMs) extracted from the original galaxy and non-galaxy images. \n",
        "- Three classifier models including CNN-Vision Transformer, ResNet50, VGG16 have investigated to work the information of original galaxy and non-galaxy images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import libraries:\n",
        "\n",
        "The list of requried libraries are sklearn, pandas, numpy, tensorflow, matplotlib, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qma9b0d_c5b1"
      },
      "outputs": [],
      "source": [
        "#Import packages\n",
        "\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.layers import (Dense, Dropout,BatchNormalization, Input, Conv1D, Flatten,\n",
        "                             MaxPooling1D)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "#Scikit_learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (roc_curve, roc_auc_score, auc, log_loss,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             accuracy_score, classification_report,\n",
        "                             ConfusionMatrixDisplay, confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two classifier models based on Zernike moments (ZMs):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute the ZMs:\n",
        "\n",
        "##### First we need to compute ZMs for galaxy and non-galaxy images. The ZEMO python package [https://pypi.org/project/ZEMO/] [https://github.com/hmddev1/ZEMO] can be used to compute Zernike moments (ZMs) for images. This package was described in the research paper [[IAJJ](https://ijaa.du.ac.ir/article_374_ad45803d737b0a7d4fc554a244229df6.pdf)].\n",
        "\n",
        "*Note: The galaxy and non-galaxy images are in RGB format. Here, we used the R channel of images. The size of original Galaxy Zoo 2 images is (424, 424) pixels, while we resized them to (200, 200) pixels. To compute ZMs we set the maximum order number $P{max} = 45$.* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ZEMO import zemo\n",
        "import cv2\n",
        "\n",
        "def calculate_zernike_moments(data_dir, image_size, zernike_order):\n",
        "        \n",
        "        ZBFSTR = zemo.zernike_bf(image_size, zernike_order, 1)\n",
        "        \n",
        "        image_files = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if filename.endswith('.jpg')]\n",
        "        \n",
        "        zernike_moments = []\n",
        "    \n",
        "        for img_path in image_files:\n",
        "            image = cv2.imread(img_path)\n",
        "            resized_image = cv2.resize(image, (image_size,image_size))\n",
        "            im = resized_image[:, :, 0]\n",
        "            Z = np.abs(zemo.zernike_mom(np.array(im), ZBFSTR))\n",
        "            zernike_moments.append(Z)\n",
        "        \n",
        "        df = pd.DataFrame(zernike_moments)\n",
        "    \n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Please download the **Data** files from [this link](https://drive.google.com/file/d/1wxmYQ8qpgaVDuD3kTeBrZlyny0IBA9wn/view?usp=drive_link)\n",
        "\n",
        "- The directoies of galaxy and non-galaxy images:\n",
        "\n",
        "        - Galaxy: /repository/Data/galaxy_nongalaxy/image/galaxy\n",
        "        - Non-Galxy: /repository/Data/galaxy_nongalaxy/image/nongalaxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "galaxy_path = r'/path/to/repository/Data/galaxy_nongalaxy/image/galaxy' \n",
        "nongalaxy_path = r'/path/to/repository/Data/galaxy_nongalaxy/image/nongalaxy' \n",
        "\n",
        "# Defult image size and zernike order. \n",
        "image_size = 200\n",
        "zernike_order = 45\n",
        "\n",
        "galaxy_zm_df = calculate_zernike_moments(galaxy_path, image_size, zernike_order)\n",
        "galaxy_zm_df.to_csv('/path/to/repository/Data/galaxy_nongalaxy/ZMs/galaxy_zms.csv')\n",
        "\n",
        "nongalaxy_zm_df = calculate_zernike_moments(nongalaxy_path, image_size, zernike_order)\n",
        "nongalaxy_zm_df.to_csv('/path/to/repository/Data/galaxy_nongalaxy/ZMs/nongalaxy_zms.csv')\n",
        "np.shape(galaxy_zm_df), np.shape(nongalaxy_zm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "*Note: Computing of ZMs for above mentioned galaxy and non-galaxy images are slightly consuming time. So, we upladed the zernike moments of both classes in this repository.*\n",
        "\n",
        "**To load the ZMs please use:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqHJYkXEc5b4",
        "outputId": "d4bad796-a51a-4908-8a5c-e01e196a2f11"
      },
      "outputs": [],
      "source": [
        "galaxy_zm = pd.read_csv('/path/to/repository/Data/galaxy_nongalaxy/ZMs/galaxy_zms.csv')\n",
        "nongalaxy_zm = pd.read_csv('/path/to/repository/Data/galaxy_nongalaxy/ZMs/nongalaxy_zms.csv')\n",
        "\n",
        "galaxy_zm.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "nongalaxy_zm.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "\n",
        "zmg = np.array(galaxy_zm)\n",
        "zmng = np.array(nongalaxy_zm)\n",
        "\n",
        "all_zm_data = np.concatenate([zmg,zmng])\n",
        "len(zmg), len(zmng), len(all_zm_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We use **\"0\"** for galaxy class labels and **\"1\"** for non-galaxy class labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhFySzRMYgzd",
        "outputId": "c15a32ef-902c-49f7-cad1-4edd548940f5"
      },
      "outputs": [],
      "source": [
        "galaxies_labels = np.zeros(len(zmg))\n",
        "nongalaxy_labels = np.ones(len(zmng))\n",
        "all_labels = np.concatenate([galaxies_labels, nongalaxy_labels])\n",
        "len(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **SVM + ZMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We split the data set into 75 percent traning set and 25 percent test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uV8388_eNlj"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(all_zm_data, all_labels, np.arange(len(all_labels)), \n",
        "                                                                                 test_size=0.25, shuffle=True, random_state=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Since, the galaxy and non-galaxy classifiers are unbalance class models, so we used the class weight in the program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = {0: len(all_zm_data) / (2*len(zmg)), 1: len(all_zm_data) / (2*len(zmng))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The **SVM** model uses radial base kernel (rbf), C = 1.5, and gamma = 'scale' to fit the model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SVC(kernel='rbf', probability=True, C=1.5, gamma='scale',class_weight=class_weights)\n",
        "gz2_training_model = model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now, we apply the test set to examine the classification algorithm. Using the predicted label by the machine on original labels, we compute the elements of the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "con = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(con)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To compare the performace of classifier with the random classifier, we calculate the **reciver operation charecterstic curve (ROC curve)**. The **area under the curve (AUC)** shows the probability of True positive rates of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "\n",
        "class_names = ['Galaxy', 'None-Galaxy']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(fpr, tpr, lw=1.5, label='ROC Curve (AUC = {:.3f})'.format(auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('The ROC Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To measure the performance metrics of classifier, we compute **(Recall, Precision, f1_score, Accuracy, TSS(True Skill Statistic))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "GNkms73218l2",
        "outputId": "59c9e276-851b-4fc3-bc4e-8badb8c687a5"
      },
      "outputs": [],
      "source": [
        "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
        "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
        "f1_score = f1_score(y_test, y_pred, average= 'weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "tss=(tp/(tp+fn))-(fp/(fp+tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1_score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"TSS:\", tss)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1D_CNN + ZMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We use **\"0\"** for galaxy class labels and **\"1\"** for non-galaxy class labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "galaxies_labels = np.zeros(len(zmg))\n",
        "nongalaxy_labels = np.ones(len(zmng))\n",
        "all_labels = np.concatenate([galaxies_labels, nongalaxy_labels])\n",
        "len(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We split the data set into 75 percent traning set and 25 percent test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(all_zm_data, all_labels, np.arange(len(all_labels)), \n",
        "                                                                                 test_size=0.25, shuffle=True, random_state=None)\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Since, the galaxy and non-galaxy classifiers are unbalance class models, so we used the class weight in the program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = {0: len(all_zm_data) / (2*len(zmg)), 1: len(all_zm_data) / (2*len(zmng))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Due to one dimentional structure of ZMs, we used one dimentional achitecture of CNN: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input value\n",
        "x = Input(shape=(all_zm_data.shape[1],1))\n",
        "\n",
        "#hidden layers\n",
        "c0 = Conv1D(256, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "b0 = BatchNormalization()(c0)\n",
        "m0 = MaxPooling1D(pool_size=2)(b0)\n",
        "d0 = Dropout(0.1)(m0)\n",
        "\n",
        "c1 = Conv1D(128, kernel_size=3, strides=2, padding=\"same\")(d0)\n",
        "b1 = BatchNormalization()(c1)\n",
        "m1 = MaxPooling1D(pool_size=2)(b1)\n",
        "d1 = Dropout(0.1)(m1)\n",
        "\n",
        "c2 = Conv1D(64, kernel_size=3, strides=2, padding=\"same\")(d1)\n",
        "b2 = BatchNormalization()(c2)\n",
        "m2 = MaxPooling1D(pool_size=2)(b2)\n",
        "d2 = Dropout(0.1)(m2)\n",
        "\n",
        "f = Flatten()(d2)\n",
        "\n",
        "# output\n",
        "de0 = Dense(64, activation='relu')(f)\n",
        "de1 = Dense(32, activation='relu')(de0)\n",
        "de2 = Dense(2, activation='softmax')(de1)\n",
        "\n",
        "model = Model(inputs=x, outputs=de2, name=\"cnn_zm_45_galaxy_nonegalaxy\")\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The **1D_CNN** model uses EarlyStopping as callback function, batch size = 64, and number of epochs = 30 to fit the model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback Function\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "b_size = 64\n",
        "e_num = 30\n",
        "\n",
        "history = model.fit(\n",
        "X_train, y_train_encoded,\n",
        "batch_size=b_size,\n",
        "epochs=e_num,\n",
        "class_weight=class_weights,\n",
        "verbose = 1,\n",
        "callbacks=es,\n",
        "validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now, we apply the test set to examine the classification algorithm. Using the predicted label by the machine on original labels, we compute the elements of the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "con = metrics.confusion_matrix(y_test, y_pred_labels)\n",
        "print(con)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To compare the performace of classifier with the random classifier, we calculate the **reciver operation charecterstic curve (ROC curve)**. The **area under the curve (AUC)** shows the probability of True positive rates of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_labels)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_labels)\n",
        "\n",
        "class_names = ['Galaxy', 'None-Galaxy']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(fpr, tpr, lw=1.5, label='ROC Curve (AUC = {:.3f})'.format(auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('The ROC Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To measure the performance metrics of classifier, we compute **(Recall, Precision, f1_score, Accuracy, TSS(True Skill Statistic))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
        "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
        "f1_score = f1_score(y_test, y_pred, average= 'weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "tss=(tp/(tp+fn))-(fp/(fp+tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1_score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"TSS:\", tss)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Three classifier models based on the original images:\n",
        "  \n",
        "- (Vision Transformer used as data augmentation tool on the Galaxy and Non-Galaxy images.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import the requried libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import packages \n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import (Dense, Dropout, Input,Conv2D, Flatten,\n",
        "                             MaxPooling2D,BatchNormalization)\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import ResNet50, VGG16\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To read the images of each class and convert to Pillow images we used the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_galaxy_images(data_dir, target_size):\n",
        "        \n",
        "        \"\"\"\n",
        "        Loads, resizes, and processes all JPG images from the specified directory.\n",
        "\n",
        "        Parameters:\n",
        "        data_dir (str): The directory containing the JPG images to be processed.\n",
        "        target_size (tuple): The target size for resizing the images, specified as (width, height).\n",
        "\n",
        "        Returns:\n",
        "        list: A list of PIL Image objects, each representing a resized and processed image.\n",
        "\n",
        "        The function performs the following steps:\n",
        "        1. Lists all JPG image files in the specified directory.\n",
        "        2. Reads each image using OpenCV.\n",
        "        3. Resizes each image to the specified target size.\n",
        "        4. Scales the pixel values and converts the image to a format compatible with PIL.\n",
        "        5. Converts each resized image to a PIL Image object.\n",
        "        6. Appends each PIL Image object to a list.\n",
        "        7. Returns the list of PIL Image objects.\n",
        "        \"\"\"\n",
        "\n",
        "        all_images = []\n",
        "\n",
        "        file_path = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if filename.endswith('.jpg')]\n",
        "\n",
        "        for img in file_path:\n",
        "            image = cv2.imread(img)\n",
        "            resized_images=cv2.resize(image, target_size)\n",
        "            resized_images = (resized_images * 255).astype(np.uint8)\n",
        "            pil_images = Image.fromarray(resized_images)\n",
        "            all_images.append(pil_images)\n",
        "\n",
        "        return all_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Please use these directories from the repository for image data:\n",
        "        \n",
        "        - galaxy: /repository/Data/galaxy_nongalaxy/image/galaxy\n",
        "        - non-galaxy: /repository/Data/galaxy_nongalaxy/image/nongalaxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "galaxy_path = r'/path/to/repository/Data/galaxy_nongalaxy/image/galaxy' \n",
        "nongalaxy_path = r'/path/to/repository/Data/galaxy_nongalaxy/image/nongalaxy' \n",
        "\n",
        "image_size = 200\n",
        "\n",
        "g_img = load_galaxy_images(galaxy_path, target_size=(image_size,image_size))\n",
        "ng_img = load_galaxy_images(nongalaxy_path, target_size=(image_size,image_size))\n",
        "\n",
        "all_data = g_img + ng_img\n",
        "np.shape(all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We define the **vision transformer** for both training and testing data sets: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# transforms for training data\n",
        "train_transform = transforms.Compose([transforms.CenterCrop(image_size),\n",
        "                                      transforms.RandomRotation(90),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),\n",
        "                                      transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.99, 1.01)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "\n",
        "\n",
        "# transforms for test data\n",
        "test_transform = transforms.Compose([transforms.CenterCrop(image_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We use **\"0\"** for galaxy class labels and **\"1\"** for non-galaxy class labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "galaxy_labels = np.zeros(len(g_img))\n",
        "nongalaxy_labels = np.ones(len(ng_img))\n",
        "\n",
        "all_labels = np.concatenate([galaxy_labels, nongalaxy_labels])\n",
        "len(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **CNN + vision transformer + original images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We split the data set into 75 percent traning set and 25 percent test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(all_data, all_labels, np.arange(len(all_labels)), \n",
        "                                                                                 test_size=0.25, shuffle=True, random_state=None)\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We apply the vision transformer for both training and testing samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer for training data\n",
        "transformed_X_train=[]\n",
        "for i in range(len(X_train)):\n",
        "  transformed_train_images = train_transform(X_train[i])\n",
        "  new_image = np.transpose(transformed_train_images, (1, 2, 0))\n",
        "  transformed_X_train.append(new_image)\n",
        "\n",
        "# Transformer for testing data\n",
        "transformed_X_test=[]\n",
        "for j in range(len(X_test)):\n",
        "  transformed_test_images = test_transform(X_test[j])\n",
        "  new_images = np.transpose(transformed_test_images, (1, 2, 0))\n",
        "  transformed_X_test.append(new_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Since, the galaxy and non-galaxy classifiers are unbalance class models, so we used the class weight in the program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = {0: len(all_data) / (2*len(g_img)), 1: len(all_data) / (2*len(ng_img))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The structure of the classic CNN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input\n",
        "x = Input(shape=(image_size,image_size,3))\n",
        "\n",
        "#hidden layers\n",
        "c0 = Conv2D(256, kernel_size=(3,3), strides=(1,1), padding=\"same\")(x)\n",
        "b0 = BatchNormalization()(c0)\n",
        "m0 = MaxPooling2D(pool_size=(2, 2))(b0)\n",
        "d0 = Dropout(0.1)(m0)\n",
        "\n",
        "c1 = Conv2D(128, kernel_size=(3,3), strides=(1,1), padding=\"same\")(m0)\n",
        "b1 = BatchNormalization()(c1)\n",
        "m1 = MaxPooling2D(pool_size=(2, 2))(b1)\n",
        "d1 = Dropout(0.1)(m1)\n",
        "\n",
        "c2 = Conv2D(64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(m1)\n",
        "b2 = BatchNormalization()(c2)\n",
        "m2 = MaxPooling2D(pool_size=(2, 2))(b2)\n",
        "d2 = Dropout(0.1)(m2)\n",
        "\n",
        "f = Flatten()(m2)\n",
        "\n",
        "# output layers\n",
        "de0 = Dense(64, activation='relu')(f)\n",
        "de1 = Dense(32, activation='relu')(de0)\n",
        "de2 = Dense(2, activation='softmax')(de1)\n",
        "\n",
        "model = Model(inputs=x, outputs=de2, name=\"cnn_transformer_galaxy_nonegalaxy\")\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The **CNN** model uses EarlyStopping as callback function, batch size = 64, and number of epochs = 30 to fit the model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback Functions\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "b_size = 64\n",
        "e_num = 30\n",
        "\n",
        "history = model.fit(\n",
        "np.array(transformed_X_train), y_train_encoded,\n",
        "batch_size=b_size,\n",
        "epochs=e_num,\n",
        "verbose = 1,\n",
        "class_weight=class_weights,\n",
        "callbacks=es,\n",
        "validation_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now, we apply the test set to examine the classification algorithm. Using the predicted label by the machine on original labels, we compute the elements of the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(np.array(transformed_X_test))\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "con = metrics.confusion_matrix(y_test, y_pred_labels)\n",
        "print(con)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To compare the performace of classifier with the random classifier, we calculate the **reciver operation charecterstic curve (ROC curve)**. The **area under the curve (AUC)** shows the probability of True positive rates of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_labels)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_labels)\n",
        "\n",
        "class_names = ['Galaxy', 'None-Galaxy']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(fpr, tpr, lw=1.5, label='ROC Curve (AUC = {:.3f})'.format(auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('The ROC Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To measure the performance metrics of classifier, we compute **(Recall, Precision, f1_score, Accuracy, TSS(True Skill Statistic))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
        "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
        "f1_score = f1_score(y_test, y_pred, average= 'weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "tss=(tp/(tp+fn))-(fp/(fp+tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1_score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"TSS:\", tss)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **ResNet50 + vision transformer + original images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We use **\"0\"** for galaxy class labels and **\"1\"** for non-galaxy class labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "galaxy_labels = np.zeros(len(g_img))\n",
        "nongalaxy_labels = np.ones(len(ng_img))\n",
        "\n",
        "all_labels = np.concatenate([galaxy_labels, nongalaxy_labels])\n",
        "len(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We split the data set into 75 percent traning set and 25 percent test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(all_data, all_labels, np.arange(len(all_labels)), \n",
        "                                                                                 test_size=0.25, shuffle=True, random_state=None)\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We apply the vision transformer for both training and testing samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer for training data\n",
        "transformed_X_train=[]\n",
        "for i in range(len(X_train)):\n",
        "  transformed_train_images = train_transform(X_train[i])\n",
        "  new_image = np.transpose(transformed_train_images, (1, 2, 0))\n",
        "  transformed_X_train.append(new_image)\n",
        "\n",
        "# Transformer for testing data\n",
        "transformed_X_test=[]\n",
        "for j in range(len(X_test)):\n",
        "  transformed_test_images = test_transform(X_test[j])\n",
        "  new_images = np.transpose(transformed_test_images, (1, 2, 0))\n",
        "  transformed_X_test.append(new_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Since, the galaxy and non-galaxy classifiers are unbalance class models, so we used the class weight in the program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = {0: len(all_data) / (2*len(g_img)), 1: len(all_data) / (2*len(ng_img))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The **ResNet50** model uses EarlyStopping as callback function, batch size = 64, and number of epochs = 30 to fit the model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining the pretrained ResNet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(64, activation='relu')(x)  # The custom layers\n",
        "output = Dense(2, activation='softmax')(x)  \n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "history = model.fit(\n",
        "np.array(transformed_X_train), y_train_encoded,\n",
        "batch_size=b_size,\n",
        "epochs=e_num,\n",
        "verbose = 1,\n",
        "callbacks=es,\n",
        "class_weight=class_weights,\n",
        "validation_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now, we apply the test set to examine the classification algorithm. Using the predicted label by the machine on original labels, we compute the elements of the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(np.array(transformed_X_test))\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "con = metrics.confusion_matrix(y_test, y_pred_labels)\n",
        "print(con)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To compare the performace of classifier with the random classifier, we calculate the **reciver operation charecterstic curve (ROC curve)**. The **area under the curve (AUC)** shows the probability of True positive rates of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_labels)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_labels)\n",
        "\n",
        "class_names = ['Galaxy', 'None-Galaxy']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(fpr, tpr, lw=1.5, label='ROC Curve (AUC = {:.3f})'.format(auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('The ROC Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To measure the performance metrics of classifier, we compute **(Recall, Precision, f1_score, Accuracy, TSS(True Skill Statistic))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
        "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
        "f1_score = f1_score(y_test, y_pred, average= 'weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "tss=(tp/(tp+fn))-(fp/(fp+tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1_score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"TSS:\", tss)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **VGG16 + vision transformer + original images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We use **\"0\"** for galaxy class labels and **\"1\"** for non-galaxy class labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "galaxy_labels = np.zeros(len(g_img))\n",
        "nongalaxy_labels = np.ones(len(ng_img))\n",
        "\n",
        "all_labels = np.concatenate([galaxy_labels, nongalaxy_labels])\n",
        "len(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We split the data set into 75 percent traning set and 25 percent test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(all_data, all_labels, np.arange(len(all_labels)), \n",
        "                                                                                 test_size=0.25, shuffle=True, random_state=None)\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We apply the vision transformer for both training and testing samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer for training data\n",
        "transformed_X_train=[]\n",
        "for i in range(len(X_train)):\n",
        "  transformed_train_images = train_transform(X_train[i])\n",
        "  new_image = np.transpose(transformed_train_images, (1, 2, 0))\n",
        "  transformed_X_train.append(new_image)\n",
        "\n",
        "# Transformer for testing data\n",
        "transformed_X_test=[]\n",
        "for j in range(len(X_test)):\n",
        "  transformed_test_images = test_transform(X_test[j])\n",
        "  new_images = np.transpose(transformed_test_images, (1, 2, 0))\n",
        "  transformed_X_test.append(new_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Since, the galaxy and non-galaxy classifiers are unbalance class models, so we used the class weight in the program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = {0: len(all_data) / (2*len(g_img)), 1: len(all_data) / (2*len(ng_img))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The **VGG16** model uses EarlyStopping as callback function, batch size = 64, and number of epochs = 30 to fit the model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining the pretrained ResNet50\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(64, activation='relu')(x)  # The custom layers\n",
        "output = Dense(2, activation='softmax')(x)  \n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "history = model.fit(\n",
        "np.array(transformed_X_train), y_train_encoded,\n",
        "batch_size=b_size,\n",
        "epochs=e_num,\n",
        "verbose = 1,\n",
        "callbacks=es,\n",
        "class_weight=class_weights,\n",
        "validation_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now, we apply the test set to examine the classification algorithm. Using the predicted label by the machine on original labels, we compute the elements of the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(np.array(transformed_X_test))\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "con = metrics.confusion_matrix(y_test, y_pred_labels)\n",
        "print(con)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To compare the performace of classifier with the random classifier, we calculate the **reciver operation charecterstic curve (ROC curve)**. The **area under the curve (AUC)** shows the probability of True positive rates of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_labels)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_labels)\n",
        "\n",
        "class_names = ['Galaxy', 'None-Galaxy']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(fpr, tpr, lw=1.5, label='ROC Curve (AUC = {:.3f})'.format(auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('The ROC Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To measure the performance metrics of classifier, we compute **(Recall, Precision, f1_score, Accuracy, TSS(True Skill Statistic))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
        "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
        "f1_score = f1_score(y_test, y_pred, average= 'weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "tss=(tp/(tp+fn))-(fp/(fp+tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1_score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"TSS:\", tss)\n",
        "print(\"AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
